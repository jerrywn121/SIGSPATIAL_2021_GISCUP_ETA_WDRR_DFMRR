{'n_cpu': 32, 'device': device(type='cuda', index=0), 'batch_size_test': 8192, 'batch_size': 4096, 'use_multi_gpu_for_train': True, 'lr': 0.001, 'min_lr': 0.0001, 'weight_decay': 0, 'display_interval': 90, 'num_epochs': 50, 'early_stopping': True, 'patience': 4, 'gradient_clipping': True, 'clipping_threshold': 1.0, 'data_dir': PosixPath('/user-data/data_16_31'), 'chk_path': 'checkpoint.chk', 'all_features': {'dense': ['distance', 'simple_eta', 'links_time_total', 'crosses_time_total'], 'sparse': ['time_slice_id_5m', 'time_slice_id_30m', 'day_of_week', 'is_holiday', 'weather'], 'seq_link': {'dense': ['links_time', 'links_ratio'], 'sparse': ['links_current_status', 'is_cross', 'num_next_links', 'num_prev_links']}, 'seq_cross': {'dense': ['crosses_time'], 'sparse': []}}, 'features_to_use': {'dense': ['distance', 'simple_eta', 'links_time_total', 'crosses_time_total'], 'sparse': ['time_slice_id_30m', 'day_of_week', 'weather'], 'seq_link': {'dense': ['links_time', 'links_ratio'], 'sparse': ['links_current_status', 'is_cross', 'num_next_links', 'num_prev_links']}, 'seq_cross': {'dense': ['crosses_time'], 'sparse': []}}, 'emb_dim': 20, 'sparse_num_emb': {'time_slice_id_5m': 288, 'time_slice_id_30m': 48, 'day_of_week': 7, 'is_holiday': 2, 'weather': 5, 'links_current_status': 5, 'is_cross': 3, 'num_next_links': 8, 'num_prev_links': 8}, 'dense_field_dims': 4, 'sparse_field_dims': [48, 7, 5], 'train_period': ['20200816', '20200827'], 'eval_period': ['20200828', '20200829'], 'test_period': ['20200830', '20200831'], 'model': 'DeepFM', 'wide': {'output_dim': 256}, 'deep': {'hidden_dims': [256, 256], 'output_dim': 256, 'batchnorm': False, 'dropout': 0.1}, 'rnn': {'use_seq_link': True, 'use_seq_cross': True, 'hidden_dim': 256, 'num_layers': 2, 'type': 'LSTM', 'bidirectional': True, 'init_from_deep': False, 'auxiliary_loss': False, 'emb_dim': 20, 'output_dim': 512}, 'head': {'head_hidden_dim': 32}}
reading data from /user-data/data_16_31
processing training set
loading files from ['20200816', '20200817', '20200818', '20200819', '20200820', '20200821', '20200822', '20200823', '20200824', '20200825', '20200826', '20200827']
{'dense': (3715897, 4), 'sparse': (3715897, 3), 'sparse_cross': 'removed, will never be used', 'seq_link_dense': 3715897, 'seq_link_sparse': 3715897, 'seq_cross_dense': 3715897, 'links_arrival_status': 'did not use', 'ata': (3715897, 1)}
processing eval set
loading files from ['20200828', '20200829']
{'dense': (632743, 4), 'sparse': (632743, 3), 'sparse_cross': 'removed, will never be used', 'seq_link_dense': 632743, 'seq_link_sparse': 632743, 'seq_cross_dense': 632743, 'links_arrival_status': 'did not use', 'ata': (632743, 1)}
will use 4 GPUs for training
loading train dataloader
loading eval dataloader

epoch: 1
batch training loss: 1.00006
batch training loss: 0.15632
batch training loss: 0.15674
batch training loss: 0.15479
batch training loss: 0.15443
batch training loss: 0.14960
batch training loss: 0.14931
batch training loss: 0.14920
batch training loss: 0.15121
batch training loss: 0.15088
batch training loss: 0.14981
epoch training time: 8.355mins
epoch eval loss: 0.15053, eval time: 1.066mins
eval loss is improved from inf to 0.15053, saving model

epoch: 2
batch training loss: 0.15122
batch training loss: 0.14928
batch training loss: 0.15031
batch training loss: 0.15026
batch training loss: 0.15267
batch training loss: 0.14891
batch training loss: 0.14981
batch training loss: 0.15239
batch training loss: 0.14990
batch training loss: 0.14801
batch training loss: 0.14893
epoch training time: 8.26mins
epoch eval loss: 0.15140, eval time: 1.0mins
Epoch     2: reducing learning rate of group 0 to 3.0000e-04.
eval loss is not improved for 1 epoch

epoch: 3
batch training loss: 0.14885
batch training loss: 0.15229
batch training loss: 0.14864
batch training loss: 0.15113
batch training loss: 0.14773
batch training loss: 0.14956
batch training loss: 0.14927
batch training loss: 0.15270
batch training loss: 0.14760
batch training loss: 0.15246
batch training loss: 0.14944
epoch training time: 8.209mins
epoch eval loss: 0.14666, eval time: 1.014mins
eval loss is improved from 0.15053 to 0.14666, saving model

epoch: 4
batch training loss: 0.15253
batch training loss: 0.14671
batch training loss: 0.14752
batch training loss: 0.15089
batch training loss: 0.14477
batch training loss: 0.14842
batch training loss: 0.14690
batch training loss: 0.14354
batch training loss: 0.14311
batch training loss: 0.14603
batch training loss: 0.14209
epoch training time: 8.216mins
epoch eval loss: 0.14070, eval time: 1.018mins
eval loss is improved from 0.14666 to 0.14070, saving model

epoch: 5
batch training loss: 0.14344
batch training loss: 0.13937
batch training loss: 0.13965
batch training loss: 0.14095
batch training loss: 0.13937
batch training loss: 0.14158
batch training loss: 0.14237
batch training loss: 0.13842
batch training loss: 0.14126
batch training loss: 0.13831
batch training loss: 0.13805
epoch training time: 8.224mins
epoch eval loss: 0.13757, eval time: 1.088mins
eval loss is improved from 0.14070 to 0.13757, saving model

epoch: 6
batch training loss: 0.13667
batch training loss: 0.14075
batch training loss: 0.13915
batch training loss: 0.14330
batch training loss: 0.13945
batch training loss: 0.14389
batch training loss: 0.14148
batch training loss: 0.13898
batch training loss: 0.13519
batch training loss: 0.13987
batch training loss: 0.13759
epoch training time: 8.196mins
epoch eval loss: 0.13685, eval time: 1.025mins
eval loss is improved from 0.13757 to 0.13685, saving model

epoch: 7
batch training loss: 0.14058
batch training loss: 0.13813
batch training loss: 0.13188
batch training loss: 0.14136
batch training loss: 0.13530
batch training loss: 0.13735
batch training loss: 0.13780
batch training loss: 0.14003
batch training loss: 0.13684
batch training loss: 0.13685
batch training loss: 0.13855
epoch training time: 8.228mins
epoch eval loss: 0.13497, eval time: 1.057mins
eval loss is improved from 0.13685 to 0.13497, saving model

epoch: 8
batch training loss: 0.13753
batch training loss: 0.13570
batch training loss: 0.14005
batch training loss: 0.13758
batch training loss: 0.13637
batch training loss: 0.13918
batch training loss: 0.13122
batch training loss: 0.14096
batch training loss: 0.13730
batch training loss: 0.13602
batch training loss: 0.13713
epoch training time: 8.259mins
epoch eval loss: 0.13434, eval time: 1.066mins
eval loss is improved from 0.13497 to 0.13434, saving model

epoch: 9
batch training loss: 0.13857
batch training loss: 0.13791
batch training loss: 0.13909
batch training loss: 0.13555
batch training loss: 0.13241
batch training loss: 0.13399
batch training loss: 0.13604
batch training loss: 0.13275
batch training loss: 0.13087
batch training loss: 0.13546
batch training loss: 0.13943
epoch training time: 8.237mins
epoch eval loss: 0.13606, eval time: 1.076mins
Epoch     9: reducing learning rate of group 0 to 1.0000e-04.
eval loss is not improved for 1 epoch

epoch: 10
batch training loss: 0.13939
batch training loss: 0.13456
batch training loss: 0.13300
batch training loss: 0.13789
batch training loss: 0.13392
batch training loss: 0.13485
batch training loss: 0.13150
batch training loss: 0.13492
batch training loss: 0.13118
batch training loss: 0.13305
batch training loss: 0.13167
epoch training time: 8.21mins
epoch eval loss: 0.13371, eval time: 1.107mins
eval loss is improved from 0.13434 to 0.13371, saving model

epoch: 11
batch training loss: 0.13051
batch training loss: 0.13219
batch training loss: 0.13237
batch training loss: 0.13471
batch training loss: 0.13460
batch training loss: 0.13510
batch training loss: 0.13237
batch training loss: 0.13555
batch training loss: 0.13315
batch training loss: 0.13484
batch training loss: 0.13035
epoch training time: 8.233mins
epoch eval loss: 0.13325, eval time: 1.041mins
eval loss is improved from 0.13371 to 0.13325, saving model

epoch: 12
batch training loss: 0.13006
batch training loss: 0.13496
batch training loss: 0.13344
batch training loss: 0.13272
batch training loss: 0.13473
batch training loss: 0.13244
batch training loss: 0.13562
batch training loss: 0.13216
batch training loss: 0.13432
batch training loss: 0.13838
batch training loss: 0.13378
epoch training time: 8.253mins
epoch eval loss: 0.13228, eval time: 1.021mins
eval loss is improved from 0.13325 to 0.13228, saving model

epoch: 13
batch training loss: 0.13291
batch training loss: 0.13360
batch training loss: 0.13418
batch training loss: 0.13616
batch training loss: 0.13428
batch training loss: 0.13281
batch training loss: 0.13342
batch training loss: 0.13307
batch training loss: 0.13581
batch training loss: 0.13053
batch training loss: 0.13238
epoch training time: 8.208mins
epoch eval loss: 0.13214, eval time: 1.106mins
eval loss is improved from 0.13228 to 0.13214, saving model

epoch: 14
batch training loss: 0.13334
batch training loss: 0.13501
batch training loss: 0.13186
batch training loss: 0.13331
batch training loss: 0.13295
batch training loss: 0.13314
batch training loss: 0.13105
batch training loss: 0.13283
batch training loss: 0.13452
batch training loss: 0.13456
batch training loss: 0.13402
epoch training time: 8.257mins
epoch eval loss: 0.13247, eval time: 1.069mins
eval loss is not improved for 1 epoch

epoch: 15
batch training loss: 0.13298
batch training loss: 0.13334
batch training loss: 0.13320
batch training loss: 0.13385
batch training loss: 0.13412
batch training loss: 0.13189
batch training loss: 0.13399
batch training loss: 0.13334
batch training loss: 0.13569
batch training loss: 0.13440
batch training loss: 0.13231
epoch training time: 8.209mins
epoch eval loss: 0.13296, eval time: 1.046mins
eval loss is not improved for 2 epoch

epoch: 16
batch training loss: 0.13289
batch training loss: 0.13346
batch training loss: 0.13506
batch training loss: 0.13628
batch training loss: 0.13462
batch training loss: 0.13095
batch training loss: 0.13221
batch training loss: 0.13205
batch training loss: 0.13046
batch training loss: 0.13497
batch training loss: 0.12953
epoch training time: 8.276mins
epoch eval loss: 0.13173, eval time: 1.019mins
eval loss is improved from 0.13214 to 0.13173, saving model

epoch: 17
batch training loss: 0.13286
batch training loss: 0.13123
batch training loss: 0.13266
batch training loss: 0.12730
batch training loss: 0.13242
batch training loss: 0.13240
batch training loss: 0.13207
batch training loss: 0.13123
batch training loss: 0.13342
batch training loss: 0.13166
batch training loss: 0.13117
epoch training time: 8.249mins
epoch eval loss: 0.13192, eval time: 1.094mins
eval loss is not improved for 1 epoch

epoch: 18
batch training loss: 0.13200
batch training loss: 0.13016
batch training loss: 0.13163
batch training loss: 0.12930
batch training loss: 0.12899
batch training loss: 0.13434
batch training loss: 0.13161
batch training loss: 0.13500
batch training loss: 0.13262
batch training loss: 0.13183
batch training loss: 0.13314
epoch training time: 8.263mins
epoch eval loss: 0.13149, eval time: 1.15mins
eval loss is improved from 0.13173 to 0.13149, saving model

epoch: 19
batch training loss: 0.12812
batch training loss: 0.12873
batch training loss: 0.13263
batch training loss: 0.12966
batch training loss: 0.13188
batch training loss: 0.13193
batch training loss: 0.13256
batch training loss: 0.13266
batch training loss: 0.13591
batch training loss: 0.13333
batch training loss: 0.13153
epoch training time: 8.27mins
epoch eval loss: 0.13244, eval time: 1.042mins
eval loss is not improved for 1 epoch

epoch: 20
batch training loss: 0.13202
batch training loss: 0.13208
batch training loss: 0.13177
batch training loss: 0.13108
batch training loss: 0.13131
batch training loss: 0.13165
batch training loss: 0.13025
batch training loss: 0.13100
batch training loss: 0.13152
batch training loss: 0.13009
batch training loss: 0.13270
epoch training time: 8.266mins
epoch eval loss: 0.13130, eval time: 1.044mins
eval loss is improved from 0.13149 to 0.13130, saving model

epoch: 21
batch training loss: 0.13028
batch training loss: 0.13410
batch training loss: 0.13625
batch training loss: 0.13168
batch training loss: 0.13270
batch training loss: 0.13386
