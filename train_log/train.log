{'n_cpu': 32, 'device': device(type='cuda', index=0), 'batch_size_test': 8192, 'batch_size': 4096, 'use_multi_gpu_for_train': True, 'lr': 0.001, 'min_lr': 0.0001, 'weight_decay': 0, 'display_interval': 90, 'num_epochs': 50, 'early_stopping': True, 'patience': 4, 'gradient_clipping': True, 'clipping_threshold': 1.0, 'data_dir': PosixPath('/user-data/data_16_31'), 'chk_path': 'checkpoint.chk', 'all_features': {'dense': ['distance', 'simple_eta', 'links_time_total', 'crosses_time_total'], 'sparse': ['time_slice_id_5m', 'time_slice_id_30m', 'day_of_week', 'is_holiday', 'weather'], 'seq_link': {'dense': ['links_time', 'links_ratio'], 'sparse': ['links_current_status', 'is_cross', 'num_next_links', 'num_prev_links']}, 'seq_cross': {'dense': ['crosses_time'], 'sparse': []}}, 'features_to_use': {'dense': ['distance', 'simple_eta', 'links_time_total', 'crosses_time_total'], 'sparse': ['time_slice_id_30m', 'day_of_week', 'weather'], 'seq_link': {'dense': ['links_time', 'links_ratio'], 'sparse': ['links_current_status', 'is_cross', 'num_next_links', 'num_prev_links']}, 'seq_cross': {'dense': ['crosses_time'], 'sparse': []}}, 'emb_dim': 20, 'sparse_num_emb': {'time_slice_id_5m': 288, 'time_slice_id_30m': 48, 'day_of_week': 7, 'is_holiday': 2, 'weather': 5, 'links_current_status': 5, 'is_cross': 3, 'num_next_links': 8, 'num_prev_links': 8}, 'dense_field_dims': 4, 'sparse_field_dims': [48, 7, 5], 'train_period': ['20200816', '20200827'], 'eval_period': ['20200828', '20200829'], 'test_period': ['20200830', '20200831'], 'model': 'DeepFM', 'wide': {'output_dim': 256}, 'deep': {'hidden_dims': [256, 256], 'output_dim': 256, 'batchnorm': False, 'dropout': 0.1}, 'rnn': {'use_seq_link': True, 'use_seq_cross': True, 'hidden_dim': 256, 'num_layers': 2, 'type': 'LSTM', 'bidirectional': True, 'init_from_deep': True, 'auxiliary_loss': True, 'emb_dim': 20, 'output_dim': 512}, 'head': {'head_hidden_dim': 32}}
reading data from /user-data/data_16_31
processing training set
loading files from ['20200816', '20200817', '20200818', '20200819', '20200820', '20200821', '20200822', '20200823', '20200824', '20200825', '20200826', '20200827']
{'dense': (3715897, 4), 'sparse': (3715897, 3), 'sparse_cross': 'removed, will never be used', 'seq_link_dense': 3715897, 'seq_link_sparse': 3715897, 'seq_cross_dense': 3715897, 'links_arrival_status': 3715897, 'ata': (3715897, 1)}
processing eval set
loading files from ['20200828', '20200829']
{'dense': (632743, 4), 'sparse': (632743, 3), 'sparse_cross': 'removed, will never be used', 'seq_link_dense': 632743, 'seq_link_sparse': 632743, 'seq_cross_dense': 632743, 'links_arrival_status': 632743, 'ata': (632743, 1)}
will use 4 GPUs for training
loading train dataloader
loading eval dataloader

epoch: 1
batch training loss: 0.99977, aux_loss: 1.37898
batch training loss: 0.15690, aux_loss: 0.22781
batch training loss: 0.15669, aux_loss: 0.22356
batch training loss: 0.15269, aux_loss: 0.22182
batch training loss: 0.14822, aux_loss: 0.23103
batch training loss: 0.14202, aux_loss: 0.22673
batch training loss: 0.14236, aux_loss: 0.21670
batch training loss: 0.13892, aux_loss: 0.22084
batch training loss: 0.14169, aux_loss: 0.22246
batch training loss: 0.14151, aux_loss: 0.21880
batch training loss: 0.14017, aux_loss: 0.22036
epoch training time: 6.935mins
epoch eval loss: 0.13683, eval time: 52.723 s
eval loss is improved from inf to 0.13683, saving model

epoch: 2
batch training loss: 0.14001, aux_loss: 0.22513
batch training loss: 0.14025, aux_loss: 0.22485
batch training loss: 0.13853, aux_loss: 0.22708
batch training loss: 0.13776, aux_loss: 0.22095
batch training loss: 0.13804, aux_loss: 0.23322
batch training loss: 0.13639, aux_loss: 0.21131
batch training loss: 0.13567, aux_loss: 0.22338
batch training loss: 0.13812, aux_loss: 0.22838
batch training loss: 0.13676, aux_loss: 0.22448
batch training loss: 0.13480, aux_loss: 0.22205
batch training loss: 0.13634, aux_loss: 0.22825
epoch training time: 6.852mins
epoch eval loss: 0.13387, eval time: 50.253 s
eval loss is improved from 0.13683 to 0.13387, saving model

epoch: 3
batch training loss: 0.13196, aux_loss: 0.20847
batch training loss: 0.13744, aux_loss: 0.22228
batch training loss: 0.13661, aux_loss: 0.22677
batch training loss: 0.13328, aux_loss: 0.22319
batch training loss: 0.13247, aux_loss: 0.21955
batch training loss: 0.13562, aux_loss: 0.22894
batch training loss: 0.13109, aux_loss: 0.22746
batch training loss: 0.13494, aux_loss: 0.22043
batch training loss: 0.13199, aux_loss: 0.21894
batch training loss: 0.13422, aux_loss: 0.22130
batch training loss: 0.13192, aux_loss: 0.21535
epoch training time: 6.895mins
epoch eval loss: 0.13142, eval time: 52.337 s
eval loss is improved from 0.13387 to 0.13142, saving model

epoch: 4
batch training loss: 0.13652, aux_loss: 0.22775
batch training loss: 0.13040, aux_loss: 0.21790
batch training loss: 0.13185, aux_loss: 0.21606
batch training loss: 0.13155, aux_loss: 0.22495
batch training loss: 0.13472, aux_loss: 0.22036
batch training loss: 0.13242, aux_loss: 0.22243
batch training loss: 0.13243, aux_loss: 0.22784
batch training loss: 0.13017, aux_loss: 0.22742
batch training loss: 0.13366, aux_loss: 0.22119
batch training loss: 0.13209, aux_loss: 0.22790
batch training loss: 0.13220, aux_loss: 0.21743
epoch training time: 6.908mins
epoch eval loss: 0.13070, eval time: 53.33 s
eval loss is improved from 0.13142 to 0.13070, saving model

epoch: 5
batch training loss: 0.12979, aux_loss: 0.22036
batch training loss: 0.12683, aux_loss: 0.21832
batch training loss: 0.12876, aux_loss: 0.22257
batch training loss: 0.13270, aux_loss: 0.22338
batch training loss: 0.12887, aux_loss: 0.22146
batch training loss: 0.13155, aux_loss: 0.21542
batch training loss: 0.13028, aux_loss: 0.22525
batch training loss: 0.12921, aux_loss: 0.22357
batch training loss: 0.12743, aux_loss: 0.23119
batch training loss: 0.12797, aux_loss: 0.22235
batch training loss: 0.13082, aux_loss: 0.22197
epoch training time: 6.89mins
epoch eval loss: 0.12982, eval time: 53.328 s
eval loss is improved from 0.13070 to 0.12982, saving model

epoch: 6
batch training loss: 0.12452, aux_loss: 0.21416
batch training loss: 0.12827, aux_loss: 0.21782
batch training loss: 0.12815, aux_loss: 0.22850
batch training loss: 0.13187, aux_loss: 0.22197
batch training loss: 0.12824, aux_loss: 0.21990
batch training loss: 0.13132, aux_loss: 0.23043
batch training loss: 0.12992, aux_loss: 0.21775
batch training loss: 0.12858, aux_loss: 0.22308
batch training loss: 0.12596, aux_loss: 0.21608
batch training loss: 0.13067, aux_loss: 0.22294
batch training loss: 0.12973, aux_loss: 0.22995
epoch training time: 6.92mins
epoch eval loss: 0.12933, eval time: 51.472 s
eval loss is improved from 0.12982 to 0.12933, saving model

epoch: 7
batch training loss: 0.13049, aux_loss: 0.22636
batch training loss: 0.12869, aux_loss: 0.21594
batch training loss: 0.12517, aux_loss: 0.22066
batch training loss: 0.12920, aux_loss: 0.21784
batch training loss: 0.12668, aux_loss: 0.22420
batch training loss: 0.12919, aux_loss: 0.21744
batch training loss: 0.12508, aux_loss: 0.24053
batch training loss: 0.12958, aux_loss: 0.23179
batch training loss: 0.12701, aux_loss: 0.23158
batch training loss: 0.12722, aux_loss: 0.21133
batch training loss: 0.12954, aux_loss: 0.22005
epoch training time: 6.897mins
epoch eval loss: 0.12895, eval time: 55.073 s
eval loss is improved from 0.12933 to 0.12895, saving model

epoch: 8
batch training loss: 0.12619, aux_loss: 0.21811
batch training loss: 0.12715, aux_loss: 0.21690
batch training loss: 0.12869, aux_loss: 0.21829
batch training loss: 0.12559, aux_loss: 0.22433
batch training loss: 0.12636, aux_loss: 0.22530
batch training loss: 0.12985, aux_loss: 0.23225
batch training loss: 0.12499, aux_loss: 0.22097
batch training loss: 0.12854, aux_loss: 0.23071
batch training loss: 0.12726, aux_loss: 0.22429
batch training loss: 0.12633, aux_loss: 0.22119
batch training loss: 0.12686, aux_loss: 0.22826
epoch training time: 6.924mins
epoch eval loss: 0.12857, eval time: 53.898 s
eval loss is improved from 0.12895 to 0.12857, saving model

epoch: 9
batch training loss: 0.12954, aux_loss: 0.22080
batch training loss: 0.12791, aux_loss: 0.22219
batch training loss: 0.12914, aux_loss: 0.23117
batch training loss: 0.12626, aux_loss: 0.21601
batch training loss: 0.12409, aux_loss: 0.21371
batch training loss: 0.12300, aux_loss: 0.21851
batch training loss: 0.12417, aux_loss: 0.21910
batch training loss: 0.12333, aux_loss: 0.22943
batch training loss: 0.12294, aux_loss: 0.21714
batch training loss: 0.12853, aux_loss: 0.22154
batch training loss: 0.12881, aux_loss: 0.22367
epoch training time: 6.857mins
epoch eval loss: 0.12882, eval time: 53.504 s
Epoch     9: reducing learning rate of group 0 to 3.0000e-04.
eval loss is not improved for 1 epoch

epoch: 10
batch training loss: 0.12566, aux_loss: 0.21264
batch training loss: 0.12339, aux_loss: 0.22166
batch training loss: 0.12110, aux_loss: 0.22779
batch training loss: 0.12717, aux_loss: 0.22596
batch training loss: 0.12376, aux_loss: 0.21506
batch training loss: 0.12474, aux_loss: 0.22154
batch training loss: 0.12197, aux_loss: 0.21440
batch training loss: 0.12395, aux_loss: 0.22205
batch training loss: 0.12132, aux_loss: 0.22219
batch training loss: 0.12233, aux_loss: 0.21586
batch training loss: 0.12190, aux_loss: 0.22417
epoch training time: 6.852mins
epoch eval loss: 0.12823, eval time: 54.433 s
eval loss is improved from 0.12857 to 0.12823, saving model

epoch: 11
batch training loss: 0.11997, aux_loss: 0.22005
batch training loss: 0.12207, aux_loss: 0.22128
batch training loss: 0.12274, aux_loss: 0.22579
batch training loss: 0.12188, aux_loss: 0.22460
batch training loss: 0.12450, aux_loss: 0.21250
batch training loss: 0.12334, aux_loss: 0.21239
batch training loss: 0.12319, aux_loss: 0.23332
batch training loss: 0.12472, aux_loss: 0.21715
batch training loss: 0.12079, aux_loss: 0.22430
batch training loss: 0.12319, aux_loss: 0.22035
batch training loss: 0.11973, aux_loss: 0.22228
epoch training time: 6.885mins
epoch eval loss: 0.12974, eval time: 53.382 s
Epoch    11: reducing learning rate of group 0 to 1.0000e-04.
eval loss is not improved for 1 epoch

epoch: 12
batch training loss: 0.12156, aux_loss: 0.22516
batch training loss: 0.12405, aux_loss: 0.21998
batch training loss: 0.12299, aux_loss: 0.22530
batch training loss: 0.12186, aux_loss: 0.22321
batch training loss: 0.12276, aux_loss: 0.21686
batch training loss: 0.12170, aux_loss: 0.21843
batch training loss: 0.12189, aux_loss: 0.22403
batch training loss: 0.12055, aux_loss: 0.22826
batch training loss: 0.12256, aux_loss: 0.23002
batch training loss: 0.12626, aux_loss: 0.21742
batch training loss: 0.12289, aux_loss: 0.22679
epoch training time: 6.852mins
epoch eval loss: 0.12771, eval time: 50.48 s
eval loss is improved from 0.12823 to 0.12771, saving model

epoch: 13
batch training loss: 0.12162, aux_loss: 0.22029
batch training loss: 0.12293, aux_loss: 0.22097
batch training loss: 0.12214, aux_loss: 0.21200
batch training loss: 0.12357, aux_loss: 0.22058
batch training loss: 0.12328, aux_loss: 0.22904
batch training loss: 0.12206, aux_loss: 0.21902
batch training loss: 0.12310, aux_loss: 0.21757
batch training loss: 0.12177, aux_loss: 0.21322
batch training loss: 0.12313, aux_loss: 0.22238
batch training loss: 0.12020, aux_loss: 0.22484
batch training loss: 0.12103, aux_loss: 0.22373
epoch training time: 6.96mins
epoch eval loss: 0.12785, eval time: 57.594 s
eval loss is not improved for 1 epoch

epoch: 14
batch training loss: 0.12292, aux_loss: 0.22839
batch training loss: 0.12133, aux_loss: 0.22500
batch training loss: 0.12128, aux_loss: 0.21450
batch training loss: 0.12181, aux_loss: 0.22206
batch training loss: 0.12146, aux_loss: 0.21878
batch training loss: 0.12145, aux_loss: 0.21557
batch training loss: 0.11922, aux_loss: 0.21822
batch training loss: 0.12275, aux_loss: 0.21623
batch training loss: 0.12093, aux_loss: 0.22219
batch training loss: 0.12268, aux_loss: 0.22506
batch training loss: 0.12244, aux_loss: 0.22071
epoch training time: 6.944mins
epoch eval loss: 0.12789, eval time: 57.12 s
eval loss is not improved for 2 epoch

epoch: 15
batch training loss: 0.12210, aux_loss: 0.22345
batch training loss: 0.12160, aux_loss: 0.22409
batch training loss: 0.12042, aux_loss: 0.23933
batch training loss: 0.12301, aux_loss: 0.21861
batch training loss: 0.12212, aux_loss: 0.22273
batch training loss: 0.12036, aux_loss: 0.22454
batch training loss: 0.12235, aux_loss: 0.22285
batch training loss: 0.12421, aux_loss: 0.20984
batch training loss: 0.12283, aux_loss: 0.21548
batch training loss: 0.12391, aux_loss: 0.21113
batch training loss: 0.11960, aux_loss: 0.22244
epoch training time: 6.959mins
epoch eval loss: 0.12844, eval time: 55.048 s
eval loss is not improved for 3 epoch

epoch: 16
batch training loss: 0.12230, aux_loss: 0.21063
batch training loss: 0.12247, aux_loss: 0.22048
batch training loss: 0.12328, aux_loss: 0.21898
batch training loss: 0.12232, aux_loss: 0.22418
batch training loss: 0.12250, aux_loss: 0.22164
batch training loss: 0.11919, aux_loss: 0.22674
batch training loss: 0.12182, aux_loss: 0.22678
batch training loss: 0.12057, aux_loss: 0.21861
batch training loss: 0.11980, aux_loss: 0.22075
batch training loss: 0.12131, aux_loss: 0.22182
batch training loss: 0.11820, aux_loss: 0.21645
epoch training time: 6.872mins
epoch eval loss: 0.12831, eval time: 57.985 s
eval loss is not improved for 4 epoch
early stopping reached, best score is 0.127714
======training done======

processing test set
loading files from ['20200830', '20200831']
{'dense': (591940, 4), 'sparse': (591940, 3), 'sparse_cross': 'removed, will never be used', 'seq_link_dense': 591940, 'seq_link_sparse': 591940, 'seq_cross_dense': 591940, 'links_arrival_status': 591940, 'ata': (591940, 1)}
loss test: 0.12773081728138264
Shutdown is processing...
