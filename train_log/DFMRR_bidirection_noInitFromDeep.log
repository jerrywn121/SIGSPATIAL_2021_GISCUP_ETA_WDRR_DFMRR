{'n_cpu': 32, 'device': device(type='cuda', index=0), 'batch_size_test': 8192, 'batch_size': 4096, 'use_multi_gpu_for_train': True, 'lr': 0.001, 'min_lr': 0.0001, 'weight_decay': 0, 'display_interval': 90, 'num_epochs': 50, 'early_stopping': True, 'patience': 4, 'gradient_clipping': True, 'clipping_threshold': 1.0, 'data_dir': PosixPath('/user-data/data_16_31'), 'chk_path': 'checkpoint.chk', 'all_features': {'dense': ['distance', 'simple_eta', 'links_time_total', 'crosses_time_total'], 'sparse': ['time_slice_id_5m', 'time_slice_id_30m', 'day_of_week', 'is_holiday', 'weather'], 'seq_link': {'dense': ['links_time', 'links_ratio'], 'sparse': ['links_current_status', 'is_cross', 'num_next_links', 'num_prev_links']}, 'seq_cross': {'dense': ['crosses_time'], 'sparse': []}}, 'features_to_use': {'dense': ['distance', 'simple_eta', 'links_time_total', 'crosses_time_total'], 'sparse': ['time_slice_id_30m', 'day_of_week', 'weather'], 'seq_link': {'dense': ['links_time', 'links_ratio'], 'sparse': ['links_current_status', 'is_cross', 'num_next_links', 'num_prev_links']}, 'seq_cross': {'dense': ['crosses_time'], 'sparse': []}}, 'emb_dim': 20, 'sparse_num_emb': {'time_slice_id_5m': 288, 'time_slice_id_30m': 48, 'day_of_week': 7, 'is_holiday': 2, 'weather': 5, 'links_current_status': 5, 'is_cross': 3, 'num_next_links': 8, 'num_prev_links': 8}, 'dense_field_dims': 4, 'sparse_field_dims': [48, 7, 5], 'train_period': ['20200816', '20200827'], 'eval_period': ['20200828', '20200829'], 'test_period': ['20200830', '20200831'], 'model': 'DeepFM', 'wide': {'output_dim': 256}, 'deep': {'hidden_dims': [256, 256], 'output_dim': 256, 'batchnorm': False, 'dropout': 0.1}, 'rnn': {'use_seq_link': True, 'use_seq_cross': True, 'hidden_dim': 256, 'num_layers': 2, 'type': 'LSTM', 'bidirectional': True, 'init_from_deep': False, 'auxiliary_loss': False, 'emb_dim': 20, 'output_dim': 512}, 'head': {'head_hidden_dim': 32}}
reading data from /user-data/data_16_31
processing training set
loading files from ['20200816', '20200817', '20200818', '20200819', '20200820', '20200821', '20200822', '20200823', '20200824', '20200825', '20200826', '20200827']
{'dense': (3715897, 4), 'sparse': (3715897, 3), 'sparse_cross': 'removed, will never be used', 'seq_link_dense': 3715897, 'seq_link_sparse': 3715897, 'seq_cross_dense': 3715897, 'links_arrival_status': 'did not use', 'ata': (3715897, 1)}
processing eval set
loading files from ['20200828', '20200829']
{'dense': (632743, 4), 'sparse': (632743, 3), 'sparse_cross': 'removed, will never be used', 'seq_link_dense': 632743, 'seq_link_sparse': 632743, 'seq_cross_dense': 632743, 'links_arrival_status': 'did not use', 'ata': (632743, 1)}
will use 4 GPUs for training
loading train dataloader
loading eval dataloader

epoch: 1
batch training loss: 1.00006
batch training loss: 0.15632
batch training loss: 0.15674
batch training loss: 0.15483
batch training loss: 0.15447
batch training loss: 0.14960
batch training loss: 0.14931
batch training loss: 0.14920
batch training loss: 0.15116
batch training loss: 0.15092
batch training loss: 0.14982
epoch training time: 8.509mins
epoch eval loss: 0.15059, eval time: 1.224mins
eval loss is improved from inf to 0.15059, saving model

epoch: 2
batch training loss: 0.15127
batch training loss: 0.14937
batch training loss: 0.15023
batch training loss: 0.15022
batch training loss: 0.15260
batch training loss: 0.14907
batch training loss: 0.14983
batch training loss: 0.15231
batch training loss: 0.14971
batch training loss: 0.14822
batch training loss: 0.15049
epoch training time: 8.475mins
epoch eval loss: 0.14927, eval time: 1.254mins
eval loss is improved from 0.15059 to 0.14927, saving model

epoch: 3
batch training loss: 0.14655
batch training loss: 0.15295
batch training loss: 0.14922
batch training loss: 0.15222
batch training loss: 0.14853
batch training loss: 0.14971
batch training loss: 0.14669
batch training loss: 0.15479
batch training loss: 0.14957
batch training loss: 0.15388
batch training loss: 0.15106
epoch training time: 8.462mins
epoch eval loss: 0.14889, eval time: 1.261mins
eval loss is improved from 0.14927 to 0.14889, saving model

epoch: 4
batch training loss: 0.15412
batch training loss: 0.14737
batch training loss: 0.14901
batch training loss: 0.15144
batch training loss: 0.14651
batch training loss: 0.15041
batch training loss: 0.15028
batch training loss: 0.14900
batch training loss: 0.15105
batch training loss: 0.15170
batch training loss: 0.15188
epoch training time: 8.489mins
epoch eval loss: 0.14711, eval time: 1.271mins
eval loss is improved from 0.14889 to 0.14711, saving model

epoch: 5
batch training loss: 0.14698
batch training loss: 0.15297
batch training loss: 0.14815
batch training loss: 0.15036
batch training loss: 0.14720
batch training loss: 0.15042
batch training loss: 0.15005
batch training loss: 0.14682
batch training loss: 0.15158
batch training loss: 0.14809
batch training loss: 0.14658
epoch training time: 8.508mins
epoch eval loss: 0.14661, eval time: 1.261mins
eval loss is improved from 0.14711 to 0.14661, saving model

epoch: 6
batch training loss: 0.14597
batch training loss: 0.14950
batch training loss: 0.14907
batch training loss: 0.15627
batch training loss: 0.15063
batch training loss: 0.15492
batch training loss: 0.15301
batch training loss: 0.14764
batch training loss: 0.14713
batch training loss: 0.14938
batch training loss: 0.14798
epoch training time: 8.488mins
epoch eval loss: 0.14628, eval time: 1.263mins
eval loss is improved from 0.14661 to 0.14628, saving model

epoch: 7
batch training loss: 0.15066
batch training loss: 0.14865
batch training loss: 0.14482
batch training loss: 0.15228
batch training loss: 0.14850
batch training loss: 0.14858
batch training loss: 0.15029
batch training loss: 0.15483
batch training loss: 0.14894
batch training loss: 0.14883
batch training loss: 0.15109
epoch training time: 8.485mins
epoch eval loss: 0.14611, eval time: 1.276mins
eval loss is improved from 0.14628 to 0.14611, saving model

epoch: 8
batch training loss: 0.14864
batch training loss: 0.14763
batch training loss: 0.15199
batch training loss: 0.14814
batch training loss: 0.15061
batch training loss: 0.15022
batch training loss: 0.14242
batch training loss: 0.15666
batch training loss: 0.14928
batch training loss: 0.14792
batch training loss: 0.15025
epoch training time: 8.492mins
epoch eval loss: 0.14608, eval time: 1.252mins
eval loss is improved from 0.14611 to 0.14608, saving model

epoch: 9
batch training loss: 0.15094
batch training loss: 0.14952
batch training loss: 0.15257
batch training loss: 0.14702
batch training loss: 0.14396
batch training loss: 0.14876
batch training loss: 0.15051
batch training loss: 0.14443
batch training loss: 0.14419
batch training loss: 0.14796
batch training loss: 0.15328
epoch training time: 8.504mins
epoch eval loss: 0.14948, eval time: 1.277mins
Epoch     9: reducing learning rate of group 0 to 3.0000e-04.
eval loss is not improved for 1 epoch

epoch: 10
batch training loss: 0.15314
batch training loss: 0.14661
batch training loss: 0.14793
batch training loss: 0.14977
batch training loss: 0.14767
batch training loss: 0.14989
batch training loss: 0.14377
batch training loss: 0.14821
batch training loss: 0.14387
batch training loss: 0.14934
batch training loss: 0.14381
epoch training time: 8.488mins
epoch eval loss: 0.14577, eval time: 1.264mins
eval loss is improved from 0.14608 to 0.14577, saving model

epoch: 11
batch training loss: 0.14354
batch training loss: 0.14538
batch training loss: 0.14658
batch training loss: 0.14768
batch training loss: 0.14815
batch training loss: 0.14725
batch training loss: 0.14395
batch training loss: 0.14586
batch training loss: 0.14394
batch training loss: 0.14218
batch training loss: 0.13613
epoch training time: 8.493mins
epoch eval loss: 0.14123, eval time: 1.264mins
eval loss is improved from 0.14577 to 0.14123, saving model

epoch: 12
batch training loss: 0.13837
batch training loss: 0.14034
batch training loss: 0.13920
batch training loss: 0.13806
batch training loss: 0.13925
batch training loss: 0.13898
batch training loss: 0.14465
batch training loss: 0.14061
batch training loss: 0.14161
batch training loss: 0.14107
batch training loss: 0.13836
epoch training time: 8.493mins
epoch eval loss: 0.13693, eval time: 1.281mins
eval loss is improved from 0.14123 to 0.13693, saving model

epoch: 13
batch training loss: 0.13863
batch training loss: 0.13732
batch training loss: 0.13687
batch training loss: 0.14278
batch training loss: 0.13844
batch training loss: 0.13730
batch training loss: 0.13774
batch training loss: 0.13641
batch training loss: 0.13765
batch training loss: 0.13568
batch training loss: 0.13705
epoch training time: 8.521mins
epoch eval loss: 0.13487, eval time: 1.271mins
eval loss is improved from 0.13693 to 0.13487, saving model

epoch: 14
batch training loss: 0.13643
batch training loss: 0.13774
batch training loss: 0.13471
batch training loss: 0.13641
batch training loss: 0.13556
batch training loss: 0.13469
batch training loss: 0.13448
batch training loss: 0.13651
batch training loss: 0.13689
batch training loss: 0.13793
batch training loss: 0.13712
epoch training time: 8.52mins
epoch eval loss: 0.13359, eval time: 1.269mins
eval loss is improved from 0.13487 to 0.13359, saving model

epoch: 15
batch training loss: 0.13555
batch training loss: 0.13562
batch training loss: 0.13501
batch training loss: 0.13728
batch training loss: 0.13766
batch training loss: 0.13253
batch training loss: 0.13706
batch training loss: 0.13813
batch training loss: 0.14008
batch training loss: 0.13736
batch training loss: 0.13317
epoch training time: 8.495mins
epoch eval loss: 0.13492, eval time: 1.263mins
Epoch    15: reducing learning rate of group 0 to 1.0000e-04.
eval loss is not improved for 1 epoch

epoch: 16
batch training loss: 0.13449
batch training loss: 0.13460
batch training loss: 0.13690
batch training loss: 0.13802
batch training loss: 0.13718
batch training loss: 0.13239
batch training loss: 0.13391
batch training loss: 0.13377
batch training loss: 0.13075
batch training loss: 0.13597
batch training loss: 0.13133
epoch training time: 8.51mins
epoch eval loss: 0.13257, eval time: 1.276mins
eval loss is improved from 0.13359 to 0.13257, saving model

epoch: 17
batch training loss: 0.13628
batch training loss: 0.13204
batch training loss: 0.13339
batch training loss: 0.12978
batch training loss: 0.13275
batch training loss: 0.13471
batch training loss: 0.13363
batch training loss: 0.13257
batch training loss: 0.13442
batch training loss: 0.13329
batch training loss: 0.13252
epoch training time: 8.513mins
epoch eval loss: 0.13280, eval time: 1.282mins
eval loss is not improved for 1 epoch

epoch: 18
batch training loss: 0.13275
batch training loss: 0.13170
batch training loss: 0.13368
batch training loss: 0.13115
batch training loss: 0.13083
batch training loss: 0.13517
batch training loss: 0.13278
batch training loss: 0.13720
batch training loss: 0.13316
batch training loss: 0.13291
batch training loss: 0.13266
epoch training time: 8.53mins
epoch eval loss: 0.13359, eval time: 1.278mins
eval loss is not improved for 2 epoch

epoch: 19
batch training loss: 0.12978
batch training loss: 0.13063
batch training loss: 0.13465
batch training loss: 0.13147
batch training loss: 0.13244
batch training loss: 0.13263
batch training loss: 0.13438
batch training loss: 0.13329
batch training loss: 0.13879
batch training loss: 0.13590
batch training loss: 0.13271
epoch training time: 8.542mins
epoch eval loss: 0.13371, eval time: 1.263mins
eval loss is not improved for 3 epoch

epoch: 20
batch training loss: 0.13306
batch training loss: 0.13286
batch training loss: 0.13338
batch training loss: 0.13195
batch training loss: 0.13363
batch training loss: 0.13167
batch training loss: 0.13153
batch training loss: 0.13244
batch training loss: 0.13229
batch training loss: 0.13067
batch training loss: 0.13417
epoch training time: 8.528mins
epoch eval loss: 0.13292, eval time: 1.271mins
eval loss is not improved for 4 epoch
early stopping reached, best score is 0.132567
======training done======

processing test set
loading files from ['20200830', '20200831']
{'dense': (591940, 4), 'sparse': (591940, 3), 'sparse_cross': 'removed, will never be used', 'seq_link_dense': 591940, 'seq_link_sparse': 591940, 'seq_cross_dense': 591940, 'links_arrival_status': 'did not use', 'ata': (591940, 1)}
loss test: 0.13250493645923792
Shutdown is processing...
