{'n_cpu': 32, 'device': device(type='cuda', index=0), 'batch_size_test': 8192, 'batch_size': 4096, 'use_multi_gpu_for_train': True, 'lr': 0.001, 'min_lr': 0.0001, 'weight_decay': 0, 'display_interval': 90, 'num_epochs': 50, 'early_stopping': True, 'patience': 4, 'gradient_clipping': True, 'clipping_threshold': 1.0, 'data_dir': PosixPath('/user-data/data_16_31'), 'chk_path': 'checkpoint.chk', 'all_features': {'dense': ['distance', 'simple_eta', 'links_time_total', 'crosses_time_total'], 'sparse': ['time_slice_id_5m', 'time_slice_id_30m', 'day_of_week', 'is_holiday', 'weather'], 'seq_link': {'dense': ['links_time', 'links_ratio'], 'sparse': ['links_current_status', 'is_cross', 'num_next_links', 'num_prev_links']}, 'seq_cross': {'dense': ['crosses_time'], 'sparse': []}}, 'features_to_use': {'dense': ['distance', 'simple_eta', 'links_time_total', 'crosses_time_total'], 'sparse': ['time_slice_id_30m', 'day_of_week', 'weather'], 'seq_link': {'dense': ['links_time', 'links_ratio'], 'sparse': ['links_current_status', 'is_cross', 'num_next_links', 'num_prev_links']}, 'seq_cross': {'dense': ['crosses_time'], 'sparse': []}}, 'emb_dim': 20, 'sparse_num_emb': {'time_slice_id_5m': 288, 'time_slice_id_30m': 48, 'day_of_week': 7, 'is_holiday': 2, 'weather': 5, 'links_current_status': 5, 'is_cross': 3, 'num_next_links': 8, 'num_prev_links': 8}, 'dense_field_dims': 4, 'sparse_field_dims': [48, 7, 5], 'train_period': ['20200816', '20200827'], 'eval_period': ['20200828', '20200829'], 'test_period': ['20200830', '20200831'], 'model': 'DeepFM', 'wide': {'output_dim': 256}, 'deep': {'hidden_dims': [256, 256], 'output_dim': 256, 'batchnorm': False, 'dropout': 0.1}, 'rnn': {'use_seq_link': True, 'use_seq_cross': True, 'hidden_dim': 256, 'num_layers': 2, 'type': 'LSTM', 'bidirectional': False, 'init_from_deep': False, 'auxiliary_loss': False, 'emb_dim': 20, 'output_dim': 256}, 'head': {'head_hidden_dim': 32}}
reading data from /user-data/data_16_31
processing training set
loading files from ['20200816', '20200817', '20200818', '20200819', '20200820', '20200821', '20200822', '20200823', '20200824', '20200825', '20200826', '20200827']
{'dense': (3715897, 4), 'sparse': (3715897, 3), 'sparse_cross': 'removed, will never be used', 'seq_link_dense': 3715897, 'seq_link_sparse': 3715897, 'seq_cross_dense': 3715897, 'links_arrival_status': 'did not use', 'ata': (3715897, 1)}
processing eval set
loading files from ['20200828', '20200829']
{'dense': (632743, 4), 'sparse': (632743, 3), 'sparse_cross': 'removed, will never be used', 'seq_link_dense': 632743, 'seq_link_sparse': 632743, 'seq_cross_dense': 632743, 'links_arrival_status': 'did not use', 'ata': (632743, 1)}
will use 4 GPUs for training
loading train dataloader
loading eval dataloader

epoch: 1
batch training loss: 0.99971
batch training loss: 0.15794
batch training loss: 0.15748
batch training loss: 0.15478
batch training loss: 0.15489
batch training loss: 0.14999
batch training loss: 0.15009
batch training loss: 0.14967
batch training loss: 0.15162
batch training loss: 0.15150
batch training loss: 0.15066
epoch training time: 4.728mins
epoch eval loss: 0.15103, eval time: 47.98 s
eval loss is improved from inf to 0.15103, saving model

epoch: 2
batch training loss: 0.15199
batch training loss: 0.14961
batch training loss: 0.15050
batch training loss: 0.15039
batch training loss: 0.15367
batch training loss: 0.14884
batch training loss: 0.14946
batch training loss: 0.15284
batch training loss: 0.15037
batch training loss: 0.14687
batch training loss: 0.14659
epoch training time: 4.469mins
epoch eval loss: 0.15352, eval time: 46.53 s
Epoch     2: reducing learning rate of group 0 to 3.0000e-04.
eval loss is not improved for 1 epoch

epoch: 3
batch training loss: 0.15247
batch training loss: 0.14870
batch training loss: 0.14524
batch training loss: 0.14553
batch training loss: 0.14285
batch training loss: 0.14681
batch training loss: 0.14435
batch training loss: 0.14696
batch training loss: 0.14287
batch training loss: 0.14688
batch training loss: 0.14442
epoch training time: 4.473mins
epoch eval loss: 0.13953, eval time: 46.058 s
eval loss is improved from 0.15103 to 0.13953, saving model

epoch: 4
batch training loss: 0.14727
batch training loss: 0.14034
batch training loss: 0.14174
batch training loss: 0.14467
batch training loss: 0.14042
batch training loss: 0.14344
batch training loss: 0.14162
batch training loss: 0.13964
batch training loss: 0.14181
batch training loss: 0.14312
batch training loss: 0.14103
epoch training time: 4.468mins
epoch eval loss: 0.13885, eval time: 46.457 s
eval loss is improved from 0.13953 to 0.13885, saving model

epoch: 5
batch training loss: 0.14149
batch training loss: 0.13832
batch training loss: 0.13817
batch training loss: 0.14027
batch training loss: 0.13950
batch training loss: 0.14174
batch training loss: 0.14219
batch training loss: 0.13864
batch training loss: 0.13968
batch training loss: 0.13872
batch training loss: 0.13811
epoch training time: 4.473mins
epoch eval loss: 0.13678, eval time: 46.597 s
eval loss is improved from 0.13885 to 0.13678, saving model

epoch: 6
batch training loss: 0.13602
batch training loss: 0.14140
batch training loss: 0.13774
batch training loss: 0.14280
batch training loss: 0.13948
batch training loss: 0.14505
batch training loss: 0.14151
batch training loss: 0.14047
batch training loss: 0.13528
batch training loss: 0.14030
batch training loss: 0.13803
epoch training time: 4.483mins
epoch eval loss: 0.13622, eval time: 46.114 s
eval loss is improved from 0.13678 to 0.13622, saving model

epoch: 7
batch training loss: 0.14174
batch training loss: 0.13779
batch training loss: 0.13345
batch training loss: 0.14193
batch training loss: 0.13689
batch training loss: 0.13757
batch training loss: 0.13836
batch training loss: 0.14094
batch training loss: 0.13795
batch training loss: 0.13741
batch training loss: 0.13919
epoch training time: 4.487mins
epoch eval loss: 0.13497, eval time: 47.63 s
eval loss is improved from 0.13622 to 0.13497, saving model

epoch: 8
batch training loss: 0.13844
batch training loss: 0.13714
batch training loss: 0.14153
batch training loss: 0.13779
batch training loss: 0.13764
batch training loss: 0.14059
batch training loss: 0.13209
batch training loss: 0.14370
batch training loss: 0.13756
batch training loss: 0.13768
batch training loss: 0.13769
epoch training time: 4.469mins
epoch eval loss: 0.13420, eval time: 47.679 s
eval loss is improved from 0.13497 to 0.13420, saving model

epoch: 9
batch training loss: 0.13875
batch training loss: 0.14037
batch training loss: 0.14059
batch training loss: 0.13614
batch training loss: 0.13367
batch training loss: 0.13624
batch training loss: 0.13332
batch training loss: 0.13307
batch training loss: 0.13224
batch training loss: 0.13692
batch training loss: 0.13890
epoch training time: 4.518mins
epoch eval loss: 0.13463, eval time: 46.931 s
Epoch     9: reducing learning rate of group 0 to 1.0000e-04.
eval loss is not improved for 1 epoch

epoch: 10
batch training loss: 0.13879
batch training loss: 0.13495
batch training loss: 0.13472
batch training loss: 0.13886
batch training loss: 0.13484
batch training loss: 0.13614
batch training loss: 0.13257
batch training loss: 0.13587
batch training loss: 0.13285
batch training loss: 0.13676
batch training loss: 0.13307
epoch training time: 4.489mins
epoch eval loss: 0.13401, eval time: 47.573 s
eval loss is improved from 0.13420 to 0.13401, saving model

epoch: 11
batch training loss: 0.13167
batch training loss: 0.13305
batch training loss: 0.13416
batch training loss: 0.13470
batch training loss: 0.13725
batch training loss: 0.13597
batch training loss: 0.13451
batch training loss: 0.13717
batch training loss: 0.13500
batch training loss: 0.13614
batch training loss: 0.13186
epoch training time: 4.513mins
epoch eval loss: 0.13374, eval time: 47.944 s
eval loss is improved from 0.13401 to 0.13374, saving model

epoch: 12
batch training loss: 0.13122
batch training loss: 0.13651
batch training loss: 0.13647
batch training loss: 0.13350
batch training loss: 0.13608
batch training loss: 0.13380
batch training loss: 0.13757
batch training loss: 0.13442
batch training loss: 0.13684
batch training loss: 0.13890
batch training loss: 0.13575
epoch training time: 4.532mins
epoch eval loss: 0.13285, eval time: 46.618 s
eval loss is improved from 0.13374 to 0.13285, saving model

epoch: 13
batch training loss: 0.13365
batch training loss: 0.13468
batch training loss: 0.13443
batch training loss: 0.13766
batch training loss: 0.13527
batch training loss: 0.13441
batch training loss: 0.13438
batch training loss: 0.13475
batch training loss: 0.13575
batch training loss: 0.13214
batch training loss: 0.13292
epoch training time: 4.513mins
epoch eval loss: 0.13360, eval time: 47.679 s
eval loss is not improved for 1 epoch

epoch: 14
batch training loss: 0.13506
batch training loss: 0.13643
batch training loss: 0.13428
batch training loss: 0.13464
batch training loss: 0.13470
batch training loss: 0.13373
batch training loss: 0.13197
batch training loss: 0.13439
batch training loss: 0.13631
batch training loss: 0.13458
batch training loss: 0.13492
epoch training time: 4.535mins
epoch eval loss: 0.13309, eval time: 49.513 s
eval loss is not improved for 2 epoch

epoch: 15
batch training loss: 0.13487
batch training loss: 0.13567
batch training loss: 0.13410
batch training loss: 0.13669
batch training loss: 0.13600
batch training loss: 0.13355
batch training loss: 0.13474
batch training loss: 0.13513
batch training loss: 0.13896
batch training loss: 0.13639
batch training loss: 0.13369
epoch training time: 4.487mins
epoch eval loss: 0.13311, eval time: 47.824 s
eval loss is not improved for 3 epoch

epoch: 16
batch training loss: 0.13395
batch training loss: 0.13514
batch training loss: 0.13674
batch training loss: 0.13760
batch training loss: 0.13614
batch training loss: 0.13302
batch training loss: 0.13402
batch training loss: 0.13356
batch training loss: 0.13199
batch training loss: 0.13646
batch training loss: 0.13133
epoch training time: 4.527mins
epoch eval loss: 0.13243, eval time: 50.445 s
eval loss is improved from 0.13285 to 0.13243, saving model

epoch: 17
batch training loss: 0.13628
batch training loss: 0.13221
batch training loss: 0.13380
batch training loss: 0.12978
batch training loss: 0.13266
batch training loss: 0.13566
batch training loss: 0.13293
batch training loss: 0.13270
batch training loss: 0.13440
batch training loss: 0.13479
batch training loss: 0.13286
epoch training time: 4.538mins
epoch eval loss: 0.13258, eval time: 47.28 s
eval loss is not improved for 1 epoch

epoch: 18
batch training loss: 0.13331
batch training loss: 0.13181
batch training loss: 0.13246
batch training loss: 0.13103
batch training loss: 0.13105
batch training loss: 0.13633
batch training loss: 0.13363
batch training loss: 0.13780
batch training loss: 0.13264
batch training loss: 0.13314
batch training loss: 0.13155
epoch training time: 4.546mins
epoch eval loss: 0.13332, eval time: 51.324 s
eval loss is not improved for 2 epoch

epoch: 19
batch training loss: 0.13107
batch training loss: 0.13090
batch training loss: 0.13379
batch training loss: 0.13110
batch training loss: 0.13313
batch training loss: 0.13246
batch training loss: 0.13413
batch training loss: 0.13438
batch training loss: 0.13819
batch training loss: 0.13523
batch training loss: 0.13312
epoch training time: 4.544mins
epoch eval loss: 0.13280, eval time: 48.439 s
eval loss is not improved for 3 epoch

epoch: 20
batch training loss: 0.13295
batch training loss: 0.13352
batch training loss: 0.13406
batch training loss: 0.13196
batch training loss: 0.13421
batch training loss: 0.13278
batch training loss: 0.13149
batch training loss: 0.13195
batch training loss: 0.13328
batch training loss: 0.13195
batch training loss: 0.13393
epoch training time: 4.529mins
epoch eval loss: 0.13252, eval time: 47.632 s
eval loss is not improved for 4 epoch
early stopping reached, best score is 0.132428
======training done======

processing test set
loading files from ['20200830', '20200831']
{'dense': (591940, 4), 'sparse': (591940, 3), 'sparse_cross': 'removed, will never be used', 'seq_link_dense': 591940, 'seq_link_sparse': 591940, 'seq_cross_dense': 591940, 'links_arrival_status': 'did not use', 'ata': (591940, 1)}
loss test: 0.13234296114890925
Shutdown is processing...
