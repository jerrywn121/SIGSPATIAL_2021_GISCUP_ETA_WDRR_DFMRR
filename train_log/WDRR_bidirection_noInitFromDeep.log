{'n_cpu': 32, 'device': device(type='cuda', index=0), 'batch_size_test': 8192, 'batch_size': 4096, 'use_multi_gpu_for_train': True, 'lr': 0.001, 'min_lr': 0.0001, 'weight_decay': 0, 'display_interval': 90, 'num_epochs': 50, 'early_stopping': True, 'patience': 4, 'gradient_clipping': True, 'clipping_threshold': 1.0, 'data_dir': PosixPath('/user-data/data_16_31'), 'chk_path': 'checkpoint.chk', 'all_features': {'dense': ['distance', 'simple_eta', 'links_time_total', 'crosses_time_total'], 'sparse': ['time_slice_id_5m', 'time_slice_id_30m', 'day_of_week', 'is_holiday', 'weather'], 'seq_link': {'dense': ['links_time', 'links_ratio'], 'sparse': ['links_current_status', 'is_cross', 'num_next_links', 'num_prev_links']}, 'seq_cross': {'dense': ['crosses_time'], 'sparse': []}}, 'features_to_use': {'dense': ['distance', 'simple_eta', 'links_time_total', 'crosses_time_total'], 'sparse': ['time_slice_id_30m', 'day_of_week', 'weather'], 'seq_link': {'dense': ['links_time', 'links_ratio'], 'sparse': ['links_current_status', 'is_cross', 'num_next_links', 'num_prev_links']}, 'seq_cross': {'dense': ['crosses_time'], 'sparse': []}}, 'emb_dim': 20, 'sparse_num_emb': {'time_slice_id_5m': 288, 'time_slice_id_30m': 48, 'day_of_week': 7, 'is_holiday': 2, 'weather': 5, 'links_current_status': 5, 'is_cross': 3, 'num_next_links': 8, 'num_prev_links': 8}, 'dense_field_dims': 4, 'sparse_field_dims': [48, 7, 5], 'train_period': ['20200816', '20200827'], 'eval_period': ['20200828', '20200829'], 'test_period': ['20200830', '20200831'], 'model': 'WDRR', 'wide': {'output_dim': 256}, 'deep': {'hidden_dims': [256, 256], 'output_dim': 256, 'batchnorm': False, 'dropout': 0.1}, 'rnn': {'use_seq_link': True, 'use_seq_cross': True, 'hidden_dim': 256, 'num_layers': 2, 'type': 'LSTM', 'bidirectional': True, 'init_from_deep': False, 'auxiliary_loss': False, 'emb_dim': 20, 'output_dim': 512}, 'head': {'head_hidden_dim': 32}}
reading data from /user-data/data_16_31
processing training set
loading files from ['20200816', '20200817', '20200818', '20200819', '20200820', '20200821', '20200822', '20200823', '20200824', '20200825', '20200826', '20200827']
{'dense': (3715897, 4), 'sparse': (3715897, 3), 'sparse_cross': 'removed, will never be used', 'seq_link_dense': 3715897, 'seq_link_sparse': 3715897, 'seq_cross_dense': 3715897, 'links_arrival_status': 'did not use', 'ata': (3715897, 1)}
processing eval set
loading files from ['20200828', '20200829']
{'dense': (632743, 4), 'sparse': (632743, 3), 'sparse_cross': 'removed, will never be used', 'seq_link_dense': 632743, 'seq_link_sparse': 632743, 'seq_cross_dense': 632743, 'links_arrival_status': 'did not use', 'ata': (632743, 1)}
will use 4 GPUs for training
loading train dataloader
loading eval dataloader

epoch: 1
batch training loss: 1.00003
batch training loss: 0.16697
batch training loss: 0.15807
batch training loss: 0.15476
batch training loss: 0.15802
batch training loss: 0.15060
batch training loss: 0.14968
batch training loss: 0.15032
batch training loss: 0.15333
batch training loss: 0.15147
batch training loss: 0.15097
epoch training time: 8.098mins
epoch eval loss: 0.15139, eval time: 55.177 s
eval loss is improved from inf to 0.15139, saving model

epoch: 2
batch training loss: 0.15211
batch training loss: 0.15045
batch training loss: 0.15238
batch training loss: 0.14974
batch training loss: 0.14989
batch training loss: 0.14798
batch training loss: 0.14946
batch training loss: 0.15205
batch training loss: 0.14972
batch training loss: 0.14805
batch training loss: 0.15186
epoch training time: 8.044mins
epoch eval loss: 0.14658, eval time: 57.743 s
eval loss is improved from 0.15139 to 0.14658, saving model

epoch: 3
batch training loss: 0.14453
batch training loss: 0.15278
batch training loss: 0.14870
batch training loss: 0.14919
batch training loss: 0.14898
batch training loss: 0.14820
batch training loss: 0.14756
batch training loss: 0.15183
batch training loss: 0.15004
batch training loss: 0.15190
batch training loss: 0.15146
epoch training time: 8.041mins
epoch eval loss: 0.14748, eval time: 56.534 s
Epoch     3: reducing learning rate of group 0 to 3.0000e-04.
eval loss is not improved for 1 epoch

epoch: 4
batch training loss: 0.15227
batch training loss: 0.14674
batch training loss: 0.14726
batch training loss: 0.15084
batch training loss: 0.14661
batch training loss: 0.15052
batch training loss: 0.14854
batch training loss: 0.14608
batch training loss: 0.14884
batch training loss: 0.14790
batch training loss: 0.14583
epoch training time: 8.036mins
epoch eval loss: 0.14313, eval time: 55.965 s
eval loss is improved from 0.14658 to 0.14313, saving model

epoch: 5
batch training loss: 0.14384
batch training loss: 0.14182
batch training loss: 0.14048
batch training loss: 0.14092
batch training loss: 0.13992
batch training loss: 0.14245
batch training loss: 0.14261
batch training loss: 0.13873
batch training loss: 0.14077
batch training loss: 0.13866
batch training loss: 0.13841
epoch training time: 8.093mins
epoch eval loss: 0.13835, eval time: 1.009mins
eval loss is improved from 0.14313 to 0.13835, saving model

epoch: 6
batch training loss: 0.13737
batch training loss: 0.14197
batch training loss: 0.13862
batch training loss: 0.14259
batch training loss: 0.14015
batch training loss: 0.14424
batch training loss: 0.14159
batch training loss: 0.13930
batch training loss: 0.13468
batch training loss: 0.13964
batch training loss: 0.13780
epoch training time: 8.118mins
epoch eval loss: 0.13693, eval time: 58.235 s
eval loss is improved from 0.13835 to 0.13693, saving model

epoch: 7
batch training loss: 0.14104
batch training loss: 0.13717
batch training loss: 0.13287
batch training loss: 0.14138
batch training loss: 0.13690
batch training loss: 0.13766
batch training loss: 0.13820
batch training loss: 0.14083
batch training loss: 0.13796
batch training loss: 0.13662
batch training loss: 0.13877
epoch training time: 8.112mins
epoch eval loss: 0.13576, eval time: 58.9 s
eval loss is improved from 0.13693 to 0.13576, saving model

epoch: 8
batch training loss: 0.13741
batch training loss: 0.13755
batch training loss: 0.14020
batch training loss: 0.13727
batch training loss: 0.13721
batch training loss: 0.13942
batch training loss: 0.13233
batch training loss: 0.14127
batch training loss: 0.13735
batch training loss: 0.13659
batch training loss: 0.13763
epoch training time: 8.113mins
epoch eval loss: 0.13488, eval time: 59.365 s
eval loss is improved from 0.13576 to 0.13488, saving model

epoch: 9
batch training loss: 0.13880
batch training loss: 0.13877
batch training loss: 0.13912
batch training loss: 0.13559
batch training loss: 0.13310
batch training loss: 0.13360
batch training loss: 0.13530
batch training loss: 0.13275
batch training loss: 0.13119
batch training loss: 0.13627
batch training loss: 0.13939
epoch training time: 8.147mins
epoch eval loss: 0.13608, eval time: 58.593 s
Epoch     9: reducing learning rate of group 0 to 1.0000e-04.
eval loss is not improved for 1 epoch

epoch: 10
batch training loss: 0.13944
batch training loss: 0.13345
batch training loss: 0.13255
batch training loss: 0.13803
batch training loss: 0.13401
batch training loss: 0.13467
batch training loss: 0.13152
batch training loss: 0.13486
batch training loss: 0.13114
batch training loss: 0.13378
batch training loss: 0.13216
epoch training time: 8.108mins
epoch eval loss: 0.13474, eval time: 58.776 s
eval loss is improved from 0.13488 to 0.13474, saving model

epoch: 11
batch training loss: 0.13084
batch training loss: 0.13228
batch training loss: 0.13200
batch training loss: 0.13461
batch training loss: 0.13524
batch training loss: 0.13460
batch training loss: 0.13243
batch training loss: 0.13533
batch training loss: 0.13235
batch training loss: 0.13517
batch training loss: 0.13063
epoch training time: 8.124mins
epoch eval loss: 0.13346, eval time: 59.242 s
eval loss is improved from 0.13474 to 0.13346, saving model

epoch: 12
batch training loss: 0.13053
batch training loss: 0.13524
batch training loss: 0.13464
batch training loss: 0.13254
batch training loss: 0.13463
batch training loss: 0.13284
batch training loss: 0.13548
batch training loss: 0.13261
batch training loss: 0.13392
batch training loss: 0.13779
batch training loss: 0.13437
epoch training time: 8.111mins
epoch eval loss: 0.13289, eval time: 59.403 s
eval loss is improved from 0.13346 to 0.13289, saving model

epoch: 13
batch training loss: 0.13228
batch training loss: 0.13399
batch training loss: 0.13355
batch training loss: 0.13586
batch training loss: 0.13357
batch training loss: 0.13292
batch training loss: 0.13366
batch training loss: 0.13356
batch training loss: 0.13457
batch training loss: 0.13011
batch training loss: 0.13287
epoch training time: 8.123mins
epoch eval loss: 0.13278, eval time: 1.015mins
eval loss is improved from 0.13289 to 0.13278, saving model

epoch: 14
batch training loss: 0.13357
batch training loss: 0.13426
batch training loss: 0.13163
batch training loss: 0.13390
batch training loss: 0.13323
batch training loss: 0.13258
batch training loss: 0.13080
batch training loss: 0.13333
batch training loss: 0.13507
batch training loss: 0.13385
batch training loss: 0.13413
epoch training time: 8.137mins
epoch eval loss: 0.13330, eval time: 59.613 s
eval loss is not improved for 1 epoch

epoch: 15
batch training loss: 0.13397
batch training loss: 0.13383
batch training loss: 0.13333
batch training loss: 0.13495
batch training loss: 0.13391
batch training loss: 0.13212
batch training loss: 0.13416
batch training loss: 0.13300
batch training loss: 0.13531
batch training loss: 0.13483
batch training loss: 0.13270
epoch training time: 8.169mins
epoch eval loss: 0.13311, eval time: 1.002mins
eval loss is not improved for 2 epoch

epoch: 16
batch training loss: 0.13248
batch training loss: 0.13338
batch training loss: 0.13462
batch training loss: 0.13560
batch training loss: 0.13368
batch training loss: 0.13124
batch training loss: 0.13323
batch training loss: 0.13176
batch training loss: 0.12950
batch training loss: 0.13490
batch training loss: 0.12947
epoch training time: 8.167mins
epoch eval loss: 0.13233, eval time: 1.017mins
eval loss is improved from 0.13278 to 0.13233, saving model

epoch: 17
batch training loss: 0.13319
batch training loss: 0.13060
batch training loss: 0.13249
batch training loss: 0.12828
batch training loss: 0.13216
batch training loss: 0.13247
batch training loss: 0.13234
batch training loss: 0.13099
batch training loss: 0.13374
batch training loss: 0.13118
batch training loss: 0.13083
epoch training time: 8.184mins
epoch eval loss: 0.13230, eval time: 1.015mins
eval loss is improved from 0.13233 to 0.13230, saving model

epoch: 18
batch training loss: 0.13201
batch training loss: 0.12982
batch training loss: 0.13121
batch training loss: 0.12971
batch training loss: 0.12815
batch training loss: 0.13493
batch training loss: 0.13172
batch training loss: 0.13542
batch training loss: 0.13206
batch training loss: 0.13223
batch training loss: 0.13281
epoch training time: 8.142mins
epoch eval loss: 0.13247, eval time: 1.019mins
eval loss is not improved for 1 epoch

epoch: 19
batch training loss: 0.12800
batch training loss: 0.12896
batch training loss: 0.13241
batch training loss: 0.12917
batch training loss: 0.13183
batch training loss: 0.13062
batch training loss: 0.13270
batch training loss: 0.13216
batch training loss: 0.13679
batch training loss: 0.13369
batch training loss: 0.13057
epoch training time: 8.183mins
epoch eval loss: 0.13335, eval time: 1.02mins
eval loss is not improved for 2 epoch

epoch: 20
batch training loss: 0.13161
batch training loss: 0.13219
batch training loss: 0.13224
batch training loss: 0.13087
batch training loss: 0.13134
batch training loss: 0.13146
batch training loss: 0.12933
batch training loss: 0.13091
batch training loss: 0.13220
batch training loss: 0.13007
batch training loss: 0.13236
epoch training time: 8.198mins
epoch eval loss: 0.13234, eval time: 1.05mins
eval loss is not improved for 3 epoch

epoch: 21
batch training loss: 0.13094
batch training loss: 0.13366
batch training loss: 0.13547
batch training loss: 0.13184
batch training loss: 0.13278
batch training loss: 0.13483
batch training loss: 0.13025
batch training loss: 0.12995
batch training loss: 0.13076
batch training loss: 0.12810
batch training loss: 0.13283
epoch training time: 8.227mins
epoch eval loss: 0.13324, eval time: 1.033mins
eval loss is not improved for 4 epoch
early stopping reached, best score is 0.132298
======training done======

processing test set
loading files from ['20200830', '20200831']
{'dense': (591940, 4), 'sparse': (591940, 3), 'sparse_cross': 'removed, will never be used', 'seq_link_dense': 591940, 'seq_link_sparse': 591940, 'seq_cross_dense': 591940, 'links_arrival_status': 'did not use', 'ata': (591940, 1)}
loss test: 0.1317614192388209
Shutdown is processing...
