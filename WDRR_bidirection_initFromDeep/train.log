{'n_cpu': 32, 'device': device(type='cuda', index=0), 'batch_size_test': 8192, 'batch_size': 4096, 'use_multi_gpu_for_train': True, 'lr': 0.001, 'min_lr': 0.0001, 'weight_decay': 0, 'display_interval': 90, 'num_epochs': 50, 'early_stopping': True, 'patience': 4, 'gradient_clipping': True, 'clipping_threshold': 1.0, 'data_dir': PosixPath('/user-data/data_16_31'), 'chk_path': 'checkpoint.chk', 'all_features': {'dense': ['distance', 'simple_eta', 'links_time_total', 'crosses_time_total'], 'sparse': ['time_slice_id_5m', 'time_slice_id_30m', 'day_of_week', 'is_holiday', 'weather'], 'seq_link': {'dense': ['links_time', 'links_ratio'], 'sparse': ['links_current_status', 'is_cross', 'num_next_links', 'num_prev_links']}, 'seq_cross': {'dense': ['crosses_time'], 'sparse': []}}, 'features_to_use': {'dense': ['distance', 'simple_eta', 'links_time_total', 'crosses_time_total'], 'sparse': ['time_slice_id_30m', 'day_of_week', 'weather'], 'seq_link': {'dense': ['links_time', 'links_ratio'], 'sparse': ['links_current_status', 'is_cross', 'num_next_links', 'num_prev_links']}, 'seq_cross': {'dense': ['crosses_time'], 'sparse': []}}, 'emb_dim': 20, 'sparse_num_emb': {'time_slice_id_5m': 288, 'time_slice_id_30m': 48, 'day_of_week': 7, 'is_holiday': 2, 'weather': 5, 'links_current_status': 5, 'is_cross': 3, 'num_next_links': 8, 'num_prev_links': 8}, 'dense_field_dims': 4, 'sparse_field_dims': [48, 7, 5], 'train_period': ['20200816', '20200827'], 'eval_period': ['20200828', '20200829'], 'test_period': ['20200830', '20200831'], 'model': 'WDRR', 'wide': {'output_dim': 256}, 'deep': {'hidden_dims': [256, 256], 'output_dim': 256, 'batchnorm': False, 'dropout': 0.1}, 'rnn': {'use_seq_link': True, 'use_seq_cross': True, 'hidden_dim': 256, 'num_layers': 2, 'type': 'LSTM', 'bidirectional': True, 'init_from_deep': True, 'auxiliary_loss': False, 'emb_dim': 20, 'output_dim': 512}, 'head': {'head_hidden_dim': 32}}
reading data from /user-data/data_16_31
processing training set
loading files from ['20200816', '20200817', '20200818', '20200819', '20200820', '20200821', '20200822', '20200823', '20200824', '20200825', '20200826', '20200827']
{'dense': (3715897, 4), 'sparse': (3715897, 3), 'sparse_cross': 'removed, will never be used', 'seq_link_dense': 3715897, 'seq_link_sparse': 3715897, 'seq_cross_dense': 3715897, 'links_arrival_status': 'did not use', 'ata': (3715897, 1)}
processing eval set
loading files from ['20200828', '20200829']
{'dense': (632743, 4), 'sparse': (632743, 3), 'sparse_cross': 'removed, will never be used', 'seq_link_dense': 632743, 'seq_link_sparse': 632743, 'seq_cross_dense': 632743, 'links_arrival_status': 'did not use', 'ata': (632743, 1)}
will use 4 GPUs for training
loading train dataloader
loading eval dataloader

epoch: 1
batch training loss: 1.00010
batch training loss: 0.16761
batch training loss: 0.15838
batch training loss: 0.15449
batch training loss: 0.15678
batch training loss: 0.15020
batch training loss: 0.14943
batch training loss: 0.15029
batch training loss: 0.14631
batch training loss: 0.14468
batch training loss: 0.14338
epoch training time: 8.297mins
epoch eval loss: 0.14251, eval time: 1.039mins
eval loss is improved from inf to 0.14251, saving model

epoch: 2
batch training loss: 0.14394
batch training loss: 0.14180
batch training loss: 0.14338
batch training loss: 0.13953
batch training loss: 0.13890
batch training loss: 0.13845
batch training loss: 0.13724
batch training loss: 0.14319
batch training loss: 0.13902
batch training loss: 0.13623
batch training loss: 0.13627
epoch training time: 8.241mins
epoch eval loss: 0.13778, eval time: 1.037mins
eval loss is improved from 0.14251 to 0.13778, saving model

epoch: 3
batch training loss: 0.13623
batch training loss: 0.13887
batch training loss: 0.13717
batch training loss: 0.13438
batch training loss: 0.13482
batch training loss: 0.13612
batch training loss: 0.13348
batch training loss: 0.13591
batch training loss: 0.13315
batch training loss: 0.13672
batch training loss: 0.13195
epoch training time: 8.236mins
epoch eval loss: 0.13242, eval time: 1.068mins
eval loss is improved from 0.13778 to 0.13242, saving model

epoch: 4
batch training loss: 0.13796
batch training loss: 0.13102
batch training loss: 0.13534
batch training loss: 0.13275
batch training loss: 0.13243
batch training loss: 0.13336
batch training loss: 0.13114
batch training loss: 0.13125
batch training loss: 0.13179
batch training loss: 0.13439
batch training loss: 0.13412
epoch training time: 8.273mins
epoch eval loss: 0.13278, eval time: 1.107mins
Epoch     4: reducing learning rate of group 0 to 3.0000e-04.
eval loss is not improved for 1 epoch

epoch: 5
batch training loss: 0.13109
batch training loss: 0.12860
batch training loss: 0.12835
batch training loss: 0.13135
batch training loss: 0.12998
batch training loss: 0.13151
batch training loss: 0.13047
batch training loss: 0.12892
batch training loss: 0.12950
batch training loss: 0.12894
batch training loss: 0.13159
epoch training time: 8.262mins
epoch eval loss: 0.13018, eval time: 1.069mins
eval loss is improved from 0.13242 to 0.13018, saving model

epoch: 6
batch training loss: 0.12560
batch training loss: 0.12959
batch training loss: 0.12759
batch training loss: 0.13277
batch training loss: 0.12974
batch training loss: 0.13231
batch training loss: 0.13028
batch training loss: 0.12916
batch training loss: 0.12818
batch training loss: 0.13128
batch training loss: 0.12934
epoch training time: 8.221mins
epoch eval loss: 0.13105, eval time: 1.084mins
Epoch     6: reducing learning rate of group 0 to 1.0000e-04.
eval loss is not improved for 1 epoch

epoch: 7
batch training loss: 0.13156
batch training loss: 0.12915
batch training loss: 0.12573
batch training loss: 0.13029
batch training loss: 0.12788
batch training loss: 0.12946
batch training loss: 0.12733
batch training loss: 0.12915
batch training loss: 0.12790
batch training loss: 0.12805
batch training loss: 0.12924
epoch training time: 8.238mins
epoch eval loss: 0.12960, eval time: 1.04mins
eval loss is improved from 0.13018 to 0.12960, saving model

epoch: 8
batch training loss: 0.12968
batch training loss: 0.12681
batch training loss: 0.13012
batch training loss: 0.12845
batch training loss: 0.12793
batch training loss: 0.13173
batch training loss: 0.12392
batch training loss: 0.12981
batch training loss: 0.12859
batch training loss: 0.12726
batch training loss: 0.12853
epoch training time: 8.259mins
epoch eval loss: 0.12947, eval time: 1.031mins
eval loss is improved from 0.12960 to 0.12947, saving model

epoch: 9
batch training loss: 0.13016
batch training loss: 0.13009
batch training loss: 0.13054
batch training loss: 0.12828
batch training loss: 0.12481
batch training loss: 0.12511
batch training loss: 0.12675
batch training loss: 0.12635
batch training loss: 0.12405
batch training loss: 0.12986
batch training loss: 0.12914
epoch training time: 8.226mins
epoch eval loss: 0.12992, eval time: 1.073mins
eval loss is not improved for 1 epoch

epoch: 10
batch training loss: 0.12915
batch training loss: 0.12713
batch training loss: 0.12622
batch training loss: 0.13211
batch training loss: 0.12681
batch training loss: 0.12819
batch training loss: 0.12528
batch training loss: 0.12839
batch training loss: 0.12591
batch training loss: 0.12616
batch training loss: 0.12547
epoch training time: 8.269mins
epoch eval loss: 0.13011, eval time: 1.046mins
eval loss is not improved for 2 epoch

epoch: 11
batch training loss: 0.12466
batch training loss: 0.12665
batch training loss: 0.12563
batch training loss: 0.12682
batch training loss: 0.12749
batch training loss: 0.12743
batch training loss: 0.12621
batch training loss: 0.12868
batch training loss: 0.12562
batch training loss: 0.12794
batch training loss: 0.12361
epoch training time: 8.24mins
epoch eval loss: 0.13036, eval time: 1.063mins
eval loss is not improved for 3 epoch

epoch: 12
batch training loss: 0.12476
batch training loss: 0.12844
batch training loss: 0.12741
batch training loss: 0.12738
batch training loss: 0.12934
batch training loss: 0.12676
batch training loss: 0.12695
batch training loss: 0.12507
batch training loss: 0.12724
batch training loss: 0.13110
batch training loss: 0.12746
epoch training time: 8.237mins
epoch eval loss: 0.12917, eval time: 1.066mins
eval loss is improved from 0.12947 to 0.12917, saving model

epoch: 13
batch training loss: 0.12649
batch training loss: 0.12834
batch training loss: 0.12694
batch training loss: 0.12858
batch training loss: 0.12695
batch training loss: 0.12617
batch training loss: 0.12682
batch training loss: 0.12662
batch training loss: 0.12918
batch training loss: 0.12518
batch training loss: 0.12633
epoch training time: 8.25mins
epoch eval loss: 0.12929, eval time: 1.077mins
eval loss is not improved for 1 epoch

epoch: 14
batch training loss: 0.12663
batch training loss: 0.12745
batch training loss: 0.12560
batch training loss: 0.12716
batch training loss: 0.12719
batch training loss: 0.12685
batch training loss: 0.12370
batch training loss: 0.12618
batch training loss: 0.12723
batch training loss: 0.12827
batch training loss: 0.12668
epoch training time: 8.234mins
epoch eval loss: 0.12989, eval time: 1.052mins
eval loss is not improved for 2 epoch

epoch: 15
batch training loss: 0.12678
batch training loss: 0.12721
batch training loss: 0.12688
batch training loss: 0.12902
batch training loss: 0.12794
batch training loss: 0.12581
batch training loss: 0.12675
batch training loss: 0.12820
batch training loss: 0.12947
batch training loss: 0.12878
batch training loss: 0.12576
epoch training time: 8.249mins
epoch eval loss: 0.13009, eval time: 1.062mins
eval loss is not improved for 3 epoch

epoch: 16
batch training loss: 0.12692
batch training loss: 0.12736
batch training loss: 0.12816
batch training loss: 0.12728
batch training loss: 0.12762
batch training loss: 0.12411
batch training loss: 0.12573
batch training loss: 0.12598
batch training loss: 0.12407
batch training loss: 0.12774
batch training loss: 0.12294
epoch training time: 8.271mins
epoch eval loss: 0.12979, eval time: 1.104mins
eval loss is not improved for 4 epoch
early stopping reached, best score is 0.129168
======training done======

processing test set
loading files from ['20200830', '20200831']
{'dense': (591940, 4), 'sparse': (591940, 3), 'sparse_cross': 'removed, will never be used', 'seq_link_dense': 591940, 'seq_link_sparse': 591940, 'seq_cross_dense': 591940, 'links_arrival_status': 'did not use', 'ata': (591940, 1)}
loss test: 0.128999514584425
Shutdown is processing...
